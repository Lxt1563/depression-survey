% !TEX root = tnnls_depression_survey.tex

\ifx\allfiles\undefined
    \input{tnnls_prefix}
\fi

\section{Auxiliary depression diagnosis method based on mixed data}

The recognition model of depression based on single-mode data has made many achievements, but it also has some limitations.
For example, gait abnormalities may also be caused by physiological diseases, physiological signals are not easy to collect, and data such as voice and facial expressions are vulnerable to the impact of the environment.
Therefore, using single-mode data alone cannot meet the needs of clinical depression diagnosis. In recent years, more and more studies have combined multiple modal data for depression assessment and achieved good results.
In short, a more accurate and robust depression recognition model can be constructed by integrating complementary information from different modalities that reflect different aspects of depression.

Most depression recognition methods based on mixed data mostly use audio, video, and text data because the international competition AVEC introduced audiovisual depression analysis in 2013. Compared with other modality data, it is easy to collect~\cite{2021Deep}.
Therefore, in this section, we mainly introduce the methods of depression recognition based on audio, video, and text.

\subsection{Mixed audiovisual and text data}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[tbp]
	\centering	
	\label{fig_hard_case1}
	\includegraphics[width=1.0\linewidth]{figures/depression/AVT.png}		
	\caption{
	Different approaches for fusion technique:(a) Early fusion method; (b) Late fusion method; (c) Hybrid fusion.
	}
	\label{AVT}
\end{figure}

%ɾ����������Ƶģ̬����������׼ȷ������
%Depressed patients usually show some abnormal behavioral expressions, such as becoming sluggish in facial expressions, frequently avoiding eye contact with others, using short sentences with a flat tone when speaking, and having slow movements of the head and body parts.
%However, these depressive features may also appear when a healthy person is in a bad mood, and not all depressed patients directly show the above abnormal depressive symptoms, which means that depressed patients do not have a certain characteristic unique to them, so it is difficult to accurately detect depression by relying on a single modal characteristic.
%In recent years, there have also been many mature studies on audio-video and text based multimodal fusion methods in depression diagnosis~\cite{2021Deep}, which have more comprehensive feature coverage and enhanced performance in depression detection compared with unimodal cue detection methods.

Multimodal data auxiliary depression diagnosis mainly predicts the risk of depression by fusing the representations of different modality data, and how to effectively fuse is a crucial step.
There are three methods to fuse different modality data: early fusion, late fusion, and hybrid fusion.
As shown in Fig \ref{AVT}, the early fusion method connects the features extracted from each mode, and predicts them after generating a single feature vector.
The late fusion method refers to the fact that each modal data is trained separately in the early stage to obtain the prediction results, and the later stage is fused by decision or integration.
The hybrid fusion method combines the early fusion output with a single mode's prediction factor to predict the results~\cite{Morales2018ALF}.
At present, most of the research is based on early fusion methods.


\subsubsection{Mixed audiovisual and text data auxiliary depression diagnosis based on traditional machine learning}
\label{sec_fquality}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Machine learning methods play a crucial role in multimodal depression recognition.
As mentioned in the previous section, the method of depression recognition based on traditional machine learning audio, video, and text focuses on feature extraction. It selects more discriminating features to predict the results.
When combining these modal data for depression analysis, feature extraction and selection is equally important.

The multimodal combination method is similar to the feature extraction method of every single mode.
It extracts video features (facial action coding system coding, local binary patterns, active appearance modeling, motion history histogram, topic modeling, etc.~\cite{2009Detecting, meng2013depression}), vocal features (pitch, fluency measures, mel-scale frequency cepstral coefficients, voice quality, measures of periodicity and symmetry, etc.~\cite{2009Detecting, fraser2016detecting} and textual features (topic modeling, part-of-speech, n-gram, speech rate, parse constituents and psycholinguistic measures, etc.~\cite{gong2017topic, morales2016speech, fraser2016detecting}). The prediction is carried out by combining the features input classification or regression model.


In addition to exploring different classification and regression methods such as LR, SVM, SVR, Nearest Neighbor Classifier, Partial Least Squares, and Gaussian Staircase Regression~\cite{2009Detecting, meng2013depression, M2014Fusion, fraser2016detecting, 2016Detecting, 2017Facial, Morales2018ALF}, some researchers have explored fusion methods.
Morales et al.~\cite{Morales2018ALF} propose a new early fusion method named Informed Early Fusion.
Unlike the simple, early fusion method for composite modes, the syntax-based early fusion method uses the relationship between syntax and suppression to help improve model performance.
Kachele et al.~\cite{M2014Fusion} perform the final fusion step using a trainable Kalman filter.
Many experimental results show that, compared with single-mode data, multimodal data combined with depression analysis has a better effect.

%\subsubsection{Audiovisual cues auxiliary depression diagnosis based on deep learning}


\subsubsection{Mixed audiovisual and text data auxiliary depression diagnosis based on deep learning}
\label{sec_fquality}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
A wide range of studies has adopted LSTM as well as hybrid models for depression recognition in deep learning methodologies.


Numerous previous studies employ LSTM to extract spatiotemporal features at various scales from audio, video, and text clips to obtain crucial temporal information for a task involving depression recognition.
Mainly, Alhanai et al.~\cite{AlHanai2018} simulate the interaction between audio and text features in an LSTM neural network model to detect depression.
Further, Qureshi et al.~\cite{qureshi2019verbal} suggest a bimodal multitasking model that includes both a speech-based and a text-based module.
The model is passed through separate Bi-directional LSTM layers with an attention mechanism. The output attention vectors are concatenated and passed to feedforward layers for depression detection, severity prediction, and symptom severity predictions. %The depression detection head has a logistic sigmoid function to output depressed-class probability estimates.
Moreover, Ray et al.~\cite{ray2019multi} propose a multi-level attention-based early fusion network that employs a bidirectional long and short-term memory network architecture and applies the attention layer to a stacked Bi-directional LSTM layer.
More importantly, it recognizes the significance of intra- and inter-modal features for predicting depression levels and fusing the patterns of audio, video, and text to predict the severity of depression.


In recent years, hybrid models attract considerable attention in research employing audiovisual data for depression recognition.
Specifically, the combination modules of the DCNN and DNN, as well as CNN and transformer.
In the combination modules of DCNN and DNN, handcrafted feature descriptors are fed into a DCNN in order for it to learn high-level global features with compact dynamic information.
A DNN is then given the learned features to forecast the scale scores. For multi-modal fusion, the estimated scale scores from the three modalities are integrated into a DNN to obtain the final scale score.
The framework initially classifies depressed participants and healthy controls using DCNN and DNN~\cite{Yang2017a,Yang2017}.
In the studies ~\cite{yang2018integrating,yang2017dcnn}, it is also utilized to predict the severity of depression, with promising results.
In the combined module of CNN and transformer, for modeling audio video, scholars used CNNs to perform convolutions over the time dimension for the participants.
Moreover, for text embedding, BERT text features are extracted using a transformer.
The features from both models are concatenated and passed to a feedforward model that predicts whether an individual is depressed.
For example, Lam et al.~\cite{lam2019context} propose a novel method that incorporates a
data augmentation procedure based on topic modeling using
transformer and deep 1D CNN for acoustic feature modeling.


\subsubsection{Performance Comparison}
%\label{sec\_fquality}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
\centering
\caption{Experimental results based on mixed data}
\label{tab9}
\renewcommand\arraystretch{1.4}
\resizebox{1\linewidth}{!}{
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|cc}
\hline
\textbf{Paper} & \textbf{Dataset} & \textbf{Method}&\textbf{Data} & \textbf{F1 Score} & \textbf{RMSE}  & \textbf{MAE} \\
 \hline
Meng et al.~\cite{meng2013depression} &AVEC2013& PLS  & AV & - & 8.72 & 10.96 \\ \hline
Kachele et al.~\cite{M2014Fusion} & AVEC2013& SVR & AV & -  & 8.97 & 10.82 \\ \hline
Niu et al.~\cite{niu2020multimodal} &  AVEC2013& CNN+LSTM & AV & - &8.16 &6.14\\ \hline
Pampouchidou et al.~\cite{2017Facial} & AVEC2013/14 & \begin{tabular}[c]{@{}c@{}}Nearest Neighbour\\ Classifier\end{tabular}                                                                                                                          & AV  & 73.00 & -                 & -   \\ \hline
Morales et al.~\cite{morales2016speech}& AVEC2014 & SVM & AT & - & 9.21 & 7.56 \\\hline
AVEC2014~\cite{10.1145/2661806.2661807} &AVEC2014 &SVR &AV & -    & 7.89 & 9.89\\ \hline
Jan et al.~\cite{jan2017artificial} &AVEC2014 &DCNN & AV & -  &7.43 &6.14\\ \hline
Chao et al.~\cite{chao2015multi} & AVEC2014 & CNN+LSTM & AV & - & 9.98 & 7.91\\\hline
Niu et al.~\cite{niu2020multimodal} &AVEC2014 &CNN+LSTM & AV & - &7.03 &5.21\\\hline


AVEC2016/2017~\cite{valstar2016avec,ringeval2017avec} & DAIC-WOZ & RF   & AV& - & 7.05 & 5.66  \\ \hline
Williamson et al.~\cite{2016Detecting}& DAIC-WOZ& \begin{tabular}[c]{@{}c@{}}Gaussian Staircase\\ Model\end{tabular}                                                                                                                          & AVT& 81.00 & 5.31  & 4.18 \\\hline
Gong et al.~\cite{gong2017topic} & DAIZ-WOZ& \begin{tabular}[c]{@{}c@{}}SGD Regression\end{tabular}                                                                                                                          & ATV & 60.00 & 4.99 & 3.96 \\\hline
\multirow{3}{*}{Morales et al.~\cite{Morales2018ALF}} & \multirow{3}{*}{DAIC-WOZ} & \multirow{3}{*}{SVM}  & AT                       &49.00    & - & - \\
& & & AV & 49.00 & -  & -  \\
& & & ATV & 49.00  & - & -  \\\hline
Alhanai et al.~\cite{al2018detecting}& DAIC-WOZ& LSTM & AT & 77.00 & 6.37 & 5.10 \\\hline
\multirow{2}{*}{Rohanian et al.~\cite{rohanian2019detecting}}  & \multirow{2}{*}{DAIC-WOZ} & \multirow{2}{*}{LSTM}  & AT & 80.00-  & 5.14 & 3.66   \\
& &  & AVT  & 81.00& 4.99 & 3.61  \\\hline
\multirow{4}{*}{Qureshi et.al.~\cite{qureshi2019verbal}} & \multirow{4}{*}{DAIC-WOZ}  & \multirow{4}{*}{LSTM}  & AV                       & -  & 5.25 & 3.89  \\
& & & VT& -  & 5.11 & 3.65 \\
&  &  & AT & -& 4.64 & 3.65  \\
& & & AVT & - & 4.14  & 3.07  \\\hline
Lau et al.~\cite{lau2021improving} & DAIC-WOZ & Bi-LSTM & AT &92.00  &3.95 &3.00\\\hline
Yang et al.~\cite{yang2017hybrid}  & DAIC-WOZ & DCNN+DNN & AVT &- &5.40 &4.36 \\ \hline
Yang et al.~\cite{yang2017multimodal} & DAIC-WOZ & DCNN+DNN & AVT & -  & 5.97  & 5.16 \\ \hline
Yang et al.~\cite{yang2018integrating}  & DAIC-WOZ & DCNN+DNN & AV & 60.00& 6.34  & 5.39 \\ \hline
Lam et al.~\cite{lam2019context} & DAIC-WOZ & CNN+Transformer & AT & 87.00 & - & -  \\\hline
Niu et al.~\cite{niu2021hcag} & DAIC-WOZ & GAN & AT & 92.00 &3.80 &2.94\\\hline
AVEC2019~\cite{ ringeval2019avec} & E-DAIC & CNN & AV & -  & 6.37 & - \\ \hline
Yin et al.~\cite{2019A}& E-DAIC & LSTM  & ATV  & - & 5.50  & -  \\ \hline
\multirow{3}{*}{Ray et al.~\cite{ray2019multi}}  & \multirow{3}{*}{\makecell{E-DAIC+\\DAIC-WOZ}} & \multirow{3}{*}{LSTM}  & VT                       & -  & 4.64 & -   \\
& & & AT& -  & 4.37 & -  \\
& &  & AVT  & -  & 4.28 & -  \\\hline
\end{tabular}
\end{threeparttable}}
		\begin{tablenotes}
			\footnotesize
			\item Results of the experiment include features from audio(A), text(T) and
			\item video(V),
			 Partial Least Square (PLS),
			 Stochastic Gradient Descent (SGD),
			\item  Bi-directional LSTM (Bi-LSTM),
			 Graph Attention Network(GAN)
		\end{tablenotes}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Table \ref{tab9} summarizes the results of the experiments on the detection of depression based on multi-modal data, including the source and name of the method, the dataset used and the evaluation criteria of the experimental results, which includes classification metrics: F1 Score, Precision, Recall, and regression metrics of depression severity by depression scale: mean absolute error (MAE), root mean square error (RMSE).
%Most studies have multiple evaluation metric for experimental results. For multiple classification metric, we choose the highest F1 score for presentation.
%We review the effectiveness of the approaches examined in Tabel~\ref{tab9} to provide greater insight into the  multi-modal auxiliary depression diagnosis methods' performance.
%The experimental metrics (\%) are taken directly from the related source articles to conduct a fair comparison.
%Comparing different methods using the same dataset, the following list of observations can be summed up:

%Table~\ref{tab9} overviews the multi-modal auxiliary depression diagnosis techniques' performance comparisons.
%From the table we obtain the following four observations:

The performance of multi-modal auxiliary depression diagnosis approaches is presented in Table~\ref{tab9}.
By comparing the performance of several approaches on the same dataset, we obtain the following four observations: 

(1) In general, the more different modality data types are combined, the better the experimental results are.
For example, combining audio, video, and text data can have better results than combining two-modality data or single-modality data.
The data of different modes can balance each modality data's shortcomings and increase the diversity of extracted features, which can choose more effective features to predict depression.

(2) SVR achieves better results on the AVEC 2013/2014 dataset than other traditional machine learning-based methods.
%It is a vital application branch of SVM, which is to find a regression plane and make all the data of a set closest to the plane.
%An epsilon-SVR with intersection kernel trained using Local Gabor Binary Pattern Three Orthogonal Planes features have been employed for the video modality.
%As implemented in Waikato Environment for Knowledge Analysis, SVR with linear kernel and Sequential Minimal Optimization training is used for the audio baseline. The feature set consists of Low-level descriptors related to energy, spectral, voicing, etc.
%Finally, to create the audio-visual baseline, predictions from both audio and video baselines are obtained by taking the mean value for audio and video.
Valstar et al. ~\cite{10.1145/2661806.2661807} believe that the good results of depression recognition may be attributed to the following reasons:
Firstly, the Local Gabor Binary Pattern Three Orthogonjal Planes features have been shown before to outperform other descriptors for human behavior analysis.
Secondly, using an average dimensional affect label over multiple subjective ratings should remove some subjectivity in the interpretation of the affective behavior and remove rater errors caused by cognitive workload effects such as fatigue.


(3) On the DAIC-WOZ dataset, Stochastic Gradient Descent Regression achieves better results in traditional machine learning based methods.
Gong et al.~\cite{gong2017topic} propose a topic modeling-based multi-modal feature vector building scheme to provide the basis for context-aware analysis and perform a grid search for the following regression models: RF regression, Stochastic Gradient Descent regression, and SVR.
The author observes that with the increase of feature numbers, Stochastic Gradient Descent and SVR models continually improve their performance while the random forest model stops improving much earlier.
When the number of selected features is 46, the Stochastic Gradient Descent regression model achieves the best performance.
The result shows that the proposed approach performs significantly better than the context-unaware method and the challenge baseline for all metrics. It can discover various temporal features that have an underlying relationship with depression and further build a model.

(4)  Summarizing the results in Table \ref{tab9}, using the same dataset and modalities, we found that the deep learning approach outperforms the traditional machine learning approach in most cases.
%In early studies of multiple audiovisual and text depression recognition based on deep learning, the LSTM is commonly used by different works to learn the discriminative patterns from both images and hand-crafted features.




\subsection{Others}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Most of the data of other modes in the auxiliary depression diagnosis use single-mode data, but a few studies combine data from multiple modes for prediction.
%Each type of brain imaging can reflect the brain of patients with depression. For example, sMRI can clearly display the anatomical structure of the brain with high spatial resolution and reflect the internal structure of the brain, while fMRI can detect the changes of cerebral blood oxygen-dependent level signals and can reflect the active state of brain neural regions.
%%Therefore, Mousavian et al.~\cite{mousavian2021depression} combine the similarity of spatial cubes of sMRI and fMRI data, extract features from them and input them into a unified machine learning classifier framework to detect depression.
%Besides, several other modal fusions have also attracted extensive attention from researchers.

A few studies have concentrated on other modal fusions in addition to the combination of audiovisual and text data.
For example,
Mousavian et al.~\cite{mousavian2021depression} combine the similarity of spatial cubes of sMRI and fMRI data, extract features from them and input them into a unified machine learning classifier framework to detect depression.
Zhang et al.~\cite{zhang2019multimodal} explore from physiological and behavioral perspectives simultaneously and fuse pervasive EEG and speech signals to make the detection of depression more objective, effective, and convenient.
Jing et al. built a supervised regression model based on individual language and natural gait data to predict depression and anxiety in patients.
Their results could be the basis of both applications and future studies on multi-source data fusion in depression analysis.





\ifx\allfiles\undefined
\input{tnnls_suffix}
\fi

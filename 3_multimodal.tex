% !TEX root = tnnls_relation_gait.tex

\ifx\allfiles\undefined
    \input{tnnls_prefix}
\fi
%\section{Depression Recognition}
\section{Depression recognition method based on multi-modal data}



As described in Section 3, most studies have been conducted to detect depression based only on single physiological indicators, such as EEG, EM, speech signals, facial expressions, text, etc.
Although single indicators can identify depression to some extent, they do not give a comprehensive representation.
For example, in case of a depressed person, when a stimulus is presented, speech signals showed a longer response time and lower pronunciation rate~\cite{alghowinem2016multimodal}, while EM  showed decreased eyebrow movement and elevated blink rates ~\cite{sobin1997psychomotor, ellgring2007non, mackintosh1983blink}; thus, it is considerably likely that both modalities are correlated.
It seems obvious that the signal from single modality provided only partial information, while a combination of different modality signals can be used to form a more realistic model for recognizing depression than the former~\cite{gupta2014multimodal}.
So, there is increasing interest in using different modalities to handle information.





%The depression recognition model based on single-mode data has achieved a lot of results
%
%
%Although these single indicators may look completely different, they can be used to describe the same phenomena. For example,
%
%Although most researchers have studied single modality, there is increasing interest in using different modalities to
%handle information. Gupta et al.[17]suggested that the signal from single modality provided only partial information, while
%a combination of different modality signals can be used to form a more realistic model for recognizing depression than
%the former.




%Further,multi-modal depression detection has also attracted significant attention from researchers[24],[25].
%Several studies have been conducted to identify depression based on voice [26]C[28], event-related potential [29], facial expressions [30], [31], EEG [7], [32] and EM [3], [33]. Though these data might seem quite different, these can be used to
%describe the same phenomena [18]. For example, in case of a depressed person, when a stimulus is presented, voice data showed a longer response time and lower pronunciation rate, while EM data showed increased blink rate and longer average blink duration [34]; thus, it is considerably likely that both modalities are correlated.
%Nevertheless, each modality has its own advantages.
%It seems obvious that multi-modal fusion of different modalities can improve classification performance, because it provides more useful information compared with using only a single modality.






%To our knowledge, there is no prior work reported in the literature related to depression recognition based on multiple physiological signals using multi-modal deep learning.
%据我们所知，此前没有文献报道过使用多模态深度学习基于多种生理信号识别抑郁症。

%The depression recognition model based on single-mode data has achieved a lot of results, but it also has some limitations. For example, abnormal gait may also be caused by physiological diseases, physiological signals are not easy to collect, and data such as voice and facial expression are highly affected by the environment. Therefore, only use data from a single modality cannot satisfy the needs of clinical depression diagnosis. In recent years, more and more studies have combined multimodal data for depression assessment and detection, and also have achieved good results.




In short, a more accurate and robust depression recognition model can be constructed by integrating complementary information from different modalities that reflect different aspects of depression.
In this section, we will generally divide depression detection based on multi-modal data into three categories: depression recognition method based on multiple electrophysiological signals, depression recognition method based on multiple brain imaging, and depression recognition method based on multiple audio and video data.

\subsection{Multiple brain imaging}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The onset of depression is related to biochemical, genetic, social and environmental factors and involves abnormalities in a variety of neurotransmitters, brain regions and loops. Brain imaging can directly reflect the abnormalities in the brain of depressed patients, and the analysis of brain imaging data can be used as an important tool to detect depression. Each type of brain imaging can reflect the depressed patient's brain, such as sMRI can clearly demonstrate the anatomical structure of the brain with high spatial resolution and reflect the structural morphology within the brain, while fMRI detects changes in the blood-oxygen-dependent level signal of the brain and can reflect the active condition of the neural regions of the brain. fMRI has a higher temporal resolution compared to sMRI, and the corresponding spatial sub The corresponding spatial representation is reduced.

In recent years, most studies have been based on single-modality brain imaging data for depression detection, and few studies have combined multiple brain imaging data from different modalities for analysis. However, combining multimodal brain imaging data can compensate for each other's deficiencies caused by different imaging reasons and can better detect depression. For example, Mousavian et al.~\cite{mousavian2021depression} considered the similarity of spatial cubes from sMRI and RS-fMRI data, from which features were extracted and fed into a unified machine learning classifier framework for depression detection.


\subsection{Multiple electrophysiological signals }


Various studies have shown that electrophysiological signals such as EEG, ECG, EOG, and EMG can reflect a person's
physiological and emotional state to varying degrees, which is helpful for clinically assisted diagnosis of depression~\cite{sornmo2005bioelectrical}.
Further, electrophysiological signals not only have the benefits of good timing and convenient operation, but also have higher generalizability and stability of the developed depression recognition model~\cite{he2020advances}.
Therefore, with the continuous development and update of machine learning, research into the depression recognition using multiple physiological signals has increasingly attracted attention.
The researches generally adopt feature level fusion or decision level fusion in depression recognition.
%特征级融合

%Due to the limited information recorded by single-modality data, more and more researchers are devoted to the study of multi-modal data fusion.



\subsubsection{Feature level fusion}



%The main principle of feature level fusion is to first perform feature extraction and feature selection on the original data to filter out the feature matrix of each modality, and then fuse the extracted feature matrix into a joint feature matrix by some fusion algorithm.

\begin{figure}[tbp]
	\centering	
	\label{multi-modal_01}\includegraphics[width=0.9\linewidth]{multi-modal_01.jpg}		
	\caption{
     Flow chart of fusion based on multiple electrophysiological signals (taking two modalities as an example): subfigures a and b represent feature level fusion and decision level fusion, respectively.
	}
	\label{multi-modal_01}
\end{figure}
Feature level fusion, which can be divided into serial and parallel forms, is the fusion of the data that results from feature extraction~\cite{haghighat2016discriminant,yang2003feature}.
Feature level fusion aims to find correlations between data from the extracted features or to evaluate the modalities, which can reduce noise interference and improve model efficiency to some extent.
Taking depression recognition based on multiple electrophysiological signals as an example, the specific process of feature level fusion is shown in Fig~\ref{multi-modal_01}.
In order to identify depressions, Zhu et al.~\cite{zhu2019multimodal} proposed a model based on feature level fusion.
And they validated the presented classification model by collecting two physiological signals, EEG and EM.
The experimental result indicated that the feature fusion method slightly improved the recognition accuracy by 1.88\%, compared with the unimodal classification approach that uses only EEG or EM.
Thus, it was concluded that the feature level fusion methods can improve the mild depression recognition accuracy, demonstrating the complementary nature of the modalities.




\subsubsection{Decision level fusion}


Decision level fusion is the process of taking the information obtained from each modality and making independent decisions, and then fusing the outcomes of these decisions in some ways.
Taking depression recognition based on multiple electrophysiological signals as an example, the specific process of decision level fusion is shown in Fig~\ref{multi-modal_01}.
Among the different fusion strategies, decision level fusion has unique advantages to fuse the output of various classifiers and getting an effective result.
It can also synthesize multi-source information and avoid mutual interference.
However,  the classification results of each modality are usually not completely reliable, and there are misclassifications.
Moreover, the classification results from different modalities of one object may have high conflict, and this is very unfavorable to the fusion result.
To solve these problems, Zhu et al.~\cite{zhu2022content}  proposed a content-based multiple evidence fusion (CBMEF) method, which fused EEG and EM data at decision level.
The method mainly included two modules, the classification performance matrix module and the dual-weight fusion module.
The classification performance matrices of different modalities were estimated by Bayesian rule based on confusion matrix and Mahalanobis distance, and the matrices were used to correct the classification results.
Then the relative conflict degree of each modality was calculated, and different weights were assigned to the above modalities at the decision fusion layer according to this conflict degree.
The idea of introducing the classification performance matrix and the dual-weight model to multimodal biosignals fusion casts a new light on the researches of depression recognition.




\subsection{Multiple audiovisual data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[tbp]
	\centering	
	\label{fig_hard_case1}\includegraphics[width=1.0\linewidth]{figures/depression/AVT.png}		
	\caption{
	Flow chart of depression detection based on multimodal behavioral data.
	}
	\label{AVT}
\end{figure}


Depressed patients usually show some abnormal behavioral expressions, such as becoming sluggish in facial expressions, frequently avoiding eye contact with others, using short sentences with a flat tone when speaking, and having slow movements of the head and body parts. However, these depressive features may also appear when a healthy person is in a bad mood, and not all depressed patients directly show the above abnormal depressive symptoms, which means that depressed patients do not have a certain characteristic unique to them, so it is difficult to accurately detect depression by relying on a single modal characteristic.
In recent years, there have also been many mature studies on audio-video based multimodal fusion methods in depression detection~\cite{2021Deep}, which have more comprehensive feature coverage and enhanced performance in depression detection compared with unimodal audio and visual cue detection methods.

Machine learning methods play a key role in multimodal depression detection.
He et al.~\cite{2019Automatic} introduced the median robust LBP-TOP (MRLBP-TOP) descriptor that can learn patterns on different scales from image sequences for depression detection.
In addition, Sadari et al.~\cite{2020Ordinal} used ordinal logistic regression for depression identification and proposed a new approach for the task.

With the development of deep learning, more and more researchers have applied deep learning-based methods to the task of depression detection based on multimodal data, such as CNN, DNN, LSTM, etc~\cite{2017DCNN, 2018Integrating, 2017Multimodal}.
Yang et al.~\cite{yang2017hybrid} designed a hybrid multimodal depression detection model based on audio-video and text data, containing an audiovisual multimodal depression recognition framework based on deep convolutional neural networks (DCNN) and deep neural networks (DNN) to predict depression severity defined by depression scales.
Furthermore, He et al.~\cite{2021Intelligent} proposed a new temporal attention (STA) architecture and multimodal attentional feature fusion (MAFF) method for learning and extracting information and features between audio and video cues to predict subjects' scores on depression scales.

To advance the development of depression detection, AVEC started to introduce the task of depression identification in 2013 to detect depression through collected audio, video, and text data, among others. In 2018, AVEC introduced the bipolar disorder (BD) sub-challenge for researchers to detect bipolar depression, based on which, Yang et.al~\cite{2018Bipolar} proposed a new architecture incorporating DNN and random forest for bipolar depression analysis. Du et al.~\cite{du2018bipolar} designed the IncepLSTM model, which combines the initial module of feature sequences and LSTM with learning multi-scale temporal patterns for BD analysis.

Moreover, audio and video data can also be combined with textual data want for depression detection~\cite{gong2017topic,2016Detecting}.
Hanai et al.~\cite{al2018detecting} simulated the interaction with audio and text features in an LSTM neural network model to detect depression.
Albert et al.~\cite{haque2018measuring} combined data from three modalities: audio, 3D video of key points of the face, and text transcriptions of patients speaking in clinical interviews. Sentences were summarized into individual embedding models using causal convolutional networks (C-CNN) for depression classification.

In the Detecting Depression with AI SubChallenge (DDS) of A VEC2019, predict the presence and severity of depression in individuals through digital biomarkers such as vocal acoustics, verbal content of speech, and facial expressions. Provides additional opportunities for multimodal depression detection.~\cite{fan2019Multi,2019Evaluating}
Yin et al.~\cite{2019A} proposed a multimodal approach with hierarchical recurrent neural structure to integrate visual, audio, and text features for depression detection.
Makiuchi et al.~\cite{2019Multimodal} designed separate models for speech and text data for analysis, and for speech patterns used deep spectral features extracted from a pre-trained VGG-16 network with a gated convolutional neural network (GCNN) followed by an LSTM layer. For text embedding BERT text features are extracted and Convolutional Neural Network (CNN) and LSTM layers are used. Finally the two modalities are combined using feature fusion and the experiments show that the multimodal fusion approach is better than the unimodal one.

\subsection{Others}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In addition to the above three types of multimodal fusions, there are a number of other modal fusions that have also attracted extensive attention from researchers.
For example,
Bruder et al.~\cite{bruder2012relationship} combined high temporal resolution EEG signals with MRI data to increase the precision of identifying depression.
Zhang et al.~\cite{zhang2019multimodal} explored from physiological and behavioral perspectives simultaneously and fused pervasive EEG and speech signals to make the detection of depression more objective, effective and convenient.
Jing et al.~\cite{jing2019different}  built a supervised regression model based on personal language and natural gait data to predict depression and anxiety in patients. Their results could be a basis of both applications and future studies on the multi-source data fusion in anxiety and depression recognition.


\subsection{Performance Comparison}
%\label{sec\_fquality}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
\caption{Experimental results based on multi-modal}
\label{tab9}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|l|l|l|ll}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{ID}} & \multicolumn{1}{c|}{\multirow{2}{*}{Multi-modal}} & \multicolumn{1}{c|}{\multirow{2}{*}{Method}}                   & \multicolumn{1}{c|}{\multirow{2}{*}{Dataset}} & \multicolumn{2}{c}{Metrics}                        \\
\multicolumn{1}{c|}{}                    & \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{}                                          & \multicolumn{1}{c|}{}                         & \multicolumn{1}{c}{RMSE} & \multicolumn{1}{c}{MAE} \\
\hline
1                                       & \multirow{12}{*}{Audio+Video+Text}               & Y ang et al. ~\cite{yang2017multimodal} & DAIC-WOZ                                     & 5.97                     & 5.16                    \\
2                                       &                                                  & Y ang et al. ~\cite{ yang2017hybrid}    & DAIC-WOZ                                     & 5.40                     & 4.35                    \\
3                                       &                                                  & Y ang et al. ~\cite{ 2017DCNN}          & DAIC-WOZ                                     & 6.34                     & 5.38                    \\
4                                       &                                                  & Y ang et al. ~\cite{ 2018Integrating}   & DAIC-WOZ                                     & 6.34                     & 5.39                    \\
5                                       &                                                  & Shi et al. ~\cite{2019A}                & E-DAIC                                       & 5.50                     & -                       \\
6                                       &                                                  & Makiuchi et al. ~\cite{2019Multimodal}  & E-DAIC                                       & 6.11                     & -                       \\
7                                       &                                                  & Fan et al. ~\cite{fan2019Multi}         & E-DAIC                                       & 5.91                     & 4.39                    \\
8                                       &                                                  & Zhang et al.~\cite{2019Evaluating}                     & E-DAIC                                       & 6.85                     & 5.84                    \\
9                                       &                                                  & Zhao et al. ~\cite{zhao2019Automatic}   & DAIC-WOZ                                     & 5.51                     & 4.20                    \\
10                                      &                                                  & Williamson et al. ~\cite{2016Detecting}                 & 6th                                          & 5.31                     & 3.34                    \\
11                                      &                                                  & Gong et al. ~\cite{gong2017topic}       & DAIC-WOZ                                     & 4.99                     & 3.96                    \\
12                                      &                                                  & AVEC2016(baseline)                                            & DAIC-WOZ                                     & 6.62                     & 5.52                    \\
13                                      & Audio+Text                                       & Alhanai et al.~\cite{al2018detecting}                  & -                                            & 6.27                     & 4.97                    \\
14                                      & \multirow{4}{*}{Audio+Video}                     & AVEC2013(baseline)                                            & AVEC2013                                     & 13.61                    & 10.88                   \\
15                                      &                                                  & AVEC2014(baseline)                                            & AVEC2014                                     & 10.86                    & 8.86                    \\
16                                      &                                                  & Niu et al. ~\cite{2020Multimodal}       & AVEC2013                                     & 7.03                     & 5.21                    \\
17                                      &                                                  & Jan et al. ~\cite{jan2017artificial}    & AVEC2014                                     & 7.43                     & 6.14                    \\
18                                      &                                                  & Melo et al. ~\cite{2019Depression2}     & AVEC2013                                     & 8.25                     & 6.30                    \\
19                                      &                                                  & Melo et al. ~\cite{2019Depression2}     & AVEC2014                                     & 8.23                     & 6.15                   \\
\hline
\end{tabular}}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Table \ref{tab9} summarizes the results of the experiments on the detection of depression based on multi-modal data, including the source and name of the method, the data set used (- means the data used were collected by ourselves), the evaluation criteria of the experimental results, including the evaluation criteria of depression severity by depression scale mean absolute error (MAE) and root mean square error (RMSE).




\ifx\allfiles\undefined
\input{tnnls_suffix}
\fi

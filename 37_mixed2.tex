% !TEX root = tnnls_depression_survey.tex

\ifx\allfiles\undefined
    \input{tnnls_prefix}
\fi

The diagnosis model of depression based on single mode data has made many achievements, but it also has some limitations.
For example, gait abnormalities may also be caused by physiological diseases, physiological signals are not easy to collect, and data such as voice and facial expressions are vulnerable to the impact of the environment.
Therefore, the use of single mode data alone cannot meet the needs of clinical depression diagnosis. In recent years, more and more studies have combined multiple modal data for depression assessment and detection, and also have achieved good results.

In short, a more accurate and robust depression diagnosis model can be constructed by integrating complementary information from different modalities that reflect different aspects of depression.
Most auxiliary depression diagnosis methods based on multimodal data mostly use audio, video and text data, because the international competition AVEC introduced audiovisual depression analysis in 2013, and compared with other, it is easy to collect.
Therefore, in this section, we mainly introduce the methods of depression diagnosis based on audio, video and text.


\subsection{Depression recognition method based on multiple audiovisual and text data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[tbp]
	\centering	
	\label{fig_hard_case1}
	\includegraphics[width=1.0\linewidth]{figures/depression/AVT.png}		
	\caption{
	Different approaches for fusion technique:(a) Early fusion method; (b) Late fusion method.
	}
	\label{AVT}
\end{figure}


Depressed patients usually show some abnormal behavioral expressions, such as becoming sluggish in facial expressions, frequently avoiding eye contact with others, using short sentences with a flat tone when speaking, and having slow movements of the head and body parts.
However, these depressive features may also appear when a healthy person is in a bad mood, and not all depressed patients directly show the above abnormal depressive symptoms, which means that depressed patients do not have a certain characteristic unique to them, so it is difficult to accurately detect depression by relying on a single modal characteristic.
In recent years, there have also been many mature studies on audio-video and text based multimodal fusion methods in depression detection~\cite{2021Deep}, which have more comprehensive feature coverage and enhanced performance in depression detection compared with unimodal cue detection methods.

There are three methods to fuse modality data for auxiliary diagnosis of depression: early fusion, late fusion and hybrid fusion.
As shown in Fig \ref{AVT}, the early fusion method connects the features extracted from each mode, and predicts them after generating a single feature vector.
The late fusion method refers to that each modal data is trained separately in the early stage to obtain the prediction results, and the later stage is fused by decision or integration.
The hybrid fusion method combines the output of early fusion with the prediction factor of single mode to predict the results~\cite{Morales2018ALF}.
At present, most of the research is based on early fusion methods.


\subsubsection{Multiple audiovisual and text depression recognition based on traditional machine learning}
\label{sec_fquality}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Machine learning methods play a key role in multimodal depression diagnosis.
As mentioned in the previous section, the auxiliary diagnosis method of depression based on traditional machine learning audio, video and text focuses on feature extraction, and selects more discriminating features to predict the results.
When combining these modal data for depression analysis, feature extraction and selection are equally important.

The multimodal combination method is similar to the feature extraction method of each single mode.
It extracts video features (facial action coding system coding, local binary patterns, active appearance modeling, motion history histogram, topic modeling, etc.~\cite{2009Detecting, meng2013depression}), vocal features (pitch, fluency measures, mel-scale frequency cepstral coefficients, voice quality, measures of periodicity and symmetry, etc.~\cite{2009Detecting, fraser2016detecting} and textual features (topic modeling, part-of-speech, n-gram, speech rate, parse constituents and psycholinguistic measures, etc.~\cite{gong2017topic, morales2016speech, fraser2016detecting}), then the prediction is carried out by combining the features input classification or regression model.


Besides the common classification and regression methods such as Logical Regression, SVM, Support Vector Regression, Nearest Neighbor Classifier, Partial Least Squares, Gaussian Staircase Regression~\cite{2009Detecting, meng2013depression, M2014Fusion, fraser2016detecting, 2016Detecting, 2017facial, Morales2018ALF}, some researchers have explored fusion methods.
Morales et al.~\cite{Morales2018ALF} proposes a new early fusion method named Informed Early Fusion.
Unlike the simple early fusion method for composite modes, the syntax based early fusion method uses the relationship between syntax and suppression to help improve model performance.
Kachele et al.~\cite{M2014Fusion} performs the final fusion step using a trainable Kalman filter.
A large number of experimental results show that, compared with single mode data, multimodal data combined with depression diagnosis has a better effect.

%\subsubsection{Audiovisual cues auxiliary depression diagnosis based on deep learning}


\subsubsection{Multiple audiovisual and text depression recognition based on deep learning}
\label{sec_fquality}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The fast development of deep learning has motivated many scientists to study related approaches for depression recognition, resulting in a promising performance compared to the hand-crafted features.
For deep learning approaches, a wide range of studies has adopted LSTM as well as hybrid models for depression recognition.


Numerous previous studies have employed LSTM to extract spatiotemporal features at various scales from audio, video, and text clips to obtain crucial temporal information for a task involving auxiliary depression diagnosis.
Mainly, Alhanai et al.~\cite{AlHanai2018} simulate the interaction between audio and text features in an LSTM neural network model to detect depression.
Further, Qureshi et al.~\cite{qureshi2019verbal} suggest a bimodal multitasking model that includes both a speech-based and a text-based module.
The model is passed through separate BiLSTM layers with an attention mechanism. The output attention vectors are concatenated and passed to feedforward layers for depression detection, severity prediction, and symptom severity predictions. The depression detection head has a logistic sigmoid function to output depressed-class probability estimates.
Moreover, Ray et al.~\cite{ray2019multi} propose a multi-level attention-based early fusion network that employs a bidirectional long and short-term memory network architecture and applies the attention layer to a stacked BLSTM layer.
More importantly, it recognizes the significance of intra- and inter-modal features for predicting depression levels and fusing the patterns of audio, video, and text to predict the severity of depression.


In recent years, hybrid models have attracted considerable attention in research employing audiovisual data for an auxiliary depression diagnosis.
Specifically, the combination modules of the deep convolutional neural network (DCNN) and deep neural network (DNN) as well as CNN and transformer.
\emph{(1) The combination modules of DCNN and DNN:}
Yang et al.~\cite{Yang2017} propose a multi-modal fusion framework composed of DCNN and DNN models. This architecture takes into account streams of audio, video, and text. For each modality, handcrafted feature descriptors are fed into a DCNN in order for it to learn high-level global features with compact dynamic information.
A DNN is then given the learnt features to forecast the scale scores. For multi-modal fusion, the estimated scale scores from the three modalities are integrated into a DNN to obtain the final scale score.
In the meantime, ~\cite{Yang2017a} presents a hybrid depression recognition system based on audiovisual and textual descriptors.
The framework initially classifies depressed participants and healthy controls using DCNN and DNN.
In the studies ~\cite{yang2018integrating,yang2017dcnn}, Yang's approach is also utilized to predict the severity of depression, with promising results.
\emph{(2) The combination modules of CNN and transformer:}
The transformer model is a relatively new network architecture
based on the attention mechanism, dispensing with recurrence and convolutions entirely.
The attention mechanism will calculate the correlation between any two words in the sequence iteratively, allowing the dependency between two words to be captured regardless of how far apart they are, so fundamentally resolving the challenge of establishing long-term dependency.
The current study analyses audio-video and text data using CNN and transformer, respectively.
For modelling audio video, scholars used CNNs to perform convolutions over the time dimension for the participants.
Moreover, for text embedding, BERT text features are extracted using a transformer.
The features from both models are concatenated and passed to a feedforward model that predicts whether an individual is depressed.
For example, Lam et al.~\cite{lam2019context} propose a novel method that incorporates a
data augmentation procedure based on topic modelling using
transformer and deep 1D convolutional neural network (CNN) for
acoustic feature modelling.










\subsubsection{Performance Comparison}
%\label{sec\_fquality}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}
\centering
\caption{Experimental results based on multiple data}
\label{tab9}
\begin{tabular}{c|c|c|c|ccc|cc}
\hline
\multirow{2}{*}{Paper} & \multirow{2}{*}{Dataset} & \multirow{2}{*}{Method}& \multirow{2}{*}{Features} & \multicolumn{3}{c|}{Classification metrics} & \multicolumn{2}{c}{Regression metrics} \\
&  & & & F1 Score & Precision & Recall & RMSE  & MAE \\ \hline
Meng et al.~\cite{meng2013depression} &AVEC2013& Partial Least Square  & A+V & - & - & - & 8.72 & 10.96 \\ \hline
Kachele et al.~\cite{M2014Fusion} & AVEC2013& Support Vector Regression & A+V & - & - & - & 8.97 & 10.82 \\ \hline
Niu et al.~\cite{niu2020multimodal} &  AVEC2013& CNN+LSTM & A+V & - & - & - &8.16 &6.14\\ \hline
Pampouchidou et al.~\cite{2017facial} & AVEC2013/14 & Nearest Neighbour Classifier & A+V  & 0.73 & 0.95& 0.59 & -                 & -   \\ \hline
Morales et al.~\cite{morales2016speech}& AVEC2014 & SVM & A+T & - & -  & - & 9.21 & 7.56 \\\hline
AVEC2014~\cite{10.1145/2661806.2661807} &AVEC2014 &Support Vector Regression&A+V & -  & -  & -  & 7.89 & 9.89\\ \hline
Jan et al.~\cite{jan2017artificial} &AVEC2014 &DCNN & A+V & - & - & - &7.43 &6.14\\ \hline
Chao et al.~\cite{chao2015multi} & AVEC2014 & CNN+LSTM & A+V & - & -  & - & 9.98 & 7.91\\\hline
Niu et al.~\cite{niu2020multimodal} &AVEC2014 &CNN+LSTM & A+V & - & - & - &7.03 &5.21\\\hline


AVEC2016/2017~\cite{valstar2016avec,ringeval2017avec} & DAIC-WOZ & RF   & A+ V& - & - & -& 7.05 & 5.66  \\ \hline
Williamson et al.~\cite{2016Detecting}& DAIC-WOZ& Gaussian Staircase Model & A+V+T& 0.81& - & - & 5.31  & 4.18 \\\hline
Gong et al.~\cite{gong2017topic} & DAIZ-WOZ& \begin{tabular}[c]{@{}c@{}}Stochastic Gradient Descent\\ Regression\end{tabular}                                                                                                                          & A+T+V & 0.60& - & - & 4.99 & 3.96 \\\hline
\multirow{3}{*}{Morales et al.~\cite{Morales2018ALF}} & \multirow{3}{*}{DAIC-WOZ} & \multirow{3}{*}{SVM}  & A+T                       & 0.49 & 0.36 & 0.78   & - & - \\
& & & A+V & 0.49& 0.36 & 0.78 & -  & -  \\
& & & A+T+V & 0.49 & 0.36 & 0.78  & - & -  \\\hline
Alhanai et al.~\cite{al2018detecting}& DAIC-WOZ& LSTM & A+T & 0.77& 0.71 & 0.83 & 6.37 & 5.10 \\\hline
\multirow{2}{*}{Rohanian et al.~\cite{rohanian2019detecting}}  & \multirow{2}{*}{DAIC-WOZ} & \multirow{2}{*}{LSTM}  & T+A & 0.80 & 0.78 & -  & 5.14 & 3.66   \\
& &  & A+V+T  & 0.81& 0.80 & - & 4.99 & 3.61  \\\hline
\multirow{4}{*}{Qureshi et.al.~\cite{qureshi2019verbal}} & \multirow{4}{*}{DAIC-WOZ}  & \multirow{4}{*}{LSTM}  & A+V                       & -  & - & - & 5.25 & 3.89  \\
& & & V+T& - & - & - & 5.11 & 3.65 \\
&  &  & A+T & - & - & -& 4.64 & 3.65  \\
& & & A+V+T & - & - & - & 4.14  & 3.07  \\\hline
Lau et al.~\cite{lau2021improving} & DAIC-WOZ & BiLSTM & A+T & 0.92 & 1.00 & 0.86 &3.95 &3.00\\\hline
Yang et al.~\cite{yang2017hybrid}  & DAIC-WOZ & DCNN+DNN & A+V+T &- &-&-&5.40 &4.36 \\ \hline
Yang et al.~\cite{yang2017multimodal} & DAIC-WOZ & DCNN+DNN & A+V+T & -  & -  & - & 5.97  & 5.16 \\ \hline
Yang et al.~\cite{yang2018integrating}  & DAIC-WOZ & DCNN+DNN & A+V & 0.60 & 0.55& 0.67& 6.34  & 5.39 \\ \hline
Lam et al.~\cite{lam2019context} & DAIC-WOZ & CNN+Transformer & A+T & 0.87 & 0.91 & 0.83 & - & -  \\\hline
Niu et al.~\cite{niu2021hcag} & DAIC-WOZ & Graph Attention Network & A+T & 0.92 & 0.92 & 0.92 &3.80 &2.94\\\hline
AVEC2019~\cite{ ringeval2019avec} & E-DAIC & CNN & A+V & - & - & - & 6.37 & - \\ \hline
Yin et al.~\cite{2019A}& E-DAIC & LSTM  & A+T+V  & - & - & - & 5.50  & -  \\ \hline
\multirow{3}{*}{Ray et al.~\cite{ray2019multi}}  & \multirow{3}{*}{E-DAIC+ DAIC-WOZ} & \multirow{3}{*}{LSTM}  & V+T                       & - & -  & -  & 4.64 & -   \\
& & & A+T& -  & -  & - & 4.37 & -  \\
& &  & A+V+T  & - & - & -  & 4.28 & -  \\\hline
\end{tabular}
		\begin{tablenotes}
			\footnotesize
			\item Results of the experiment include features from audio(A), text(T) and video(V).
		\end{tablenotes}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Table \ref{tab9} summarizes the results of the experiments on the detection of depression based on multi-modal data, including the source and name of the method, the dataset used and the evaluation criteria of the experimental results, which includes classification metrics: F1 Score, Precision, Recall, and regression metrics of depression severity by depression scale: mean absolute error (MAE), root mean square error (RMSE).
Most studies have multiple evaluation metric for experimental results. For multiple classification metric, we choose the highest F1 score for presentation.

Comparing different methods using the same dataset, the following list of observations can be summed up:

(1) In general, the more different modality data types are combined, the better experimental results are.
For example, combining audio, video and text data can better results than combining two modality data or single modality data.
The data of different modes can balance each modality data's shortcomings and increase the diversity of extracted features, which can choose more effective features to predict depression.

(2) On the AVEC 2013/2014 dataset, Support Vector Regression achieves better results in traditional machine learning based methods.
It is an important application branch of SVM, which is to find a regression plane and make all the data of a set closest to the plane.
For the video modality , an epsilon-Support Vector Regression with intersection kernel trained using  Local Gabor Binary Pattern Three Orthogonal Planes features has been employed.
Support Vector Regression with linear kernel and Sequential Minimal Optimization training as implemented in Waikato Environment for Knowledge Analysis was used for the audio baseline, and feature set consists of Low level descriptors related to energy, spectral, voicing, etc.
Finally, in order to create the audio-visual baseline, predictions from both audio and video baselines were obtained by taking the mean value for audio and video.
The author~\cite{10.1145/2661806.2661807} believes that the good results of depression diagnosis may be attributed to the following reasons:
Firstly, the Local Gabor Binary Pattern Three Orthogonal Planes features have been shown before to outperform other descriptors for human behaviour analysis.
Secondly, using an average dimensional affect label over multiple subjective ratings should remove some of the subjectivity of the interpretation of the affective behaviour, and remove rater errors caused by cognitive workload effects such as fatigue.


(3) On the DAIC-WOZ dataset, Stochastic Gradient Descent Regression achieves better results in traditional machine learning based methods.
Gong et al.~\cite{gong2017topic} propose a topic modeling based multi-modal feature vector building scheme to provide the basis for context-aware analysis, and perform a grid search for the following regression models: Random Forest regression, Stochastic Gradient Descent regression, and Support Vector Regression.
Author observe that with the increase of feature numbers, Stochastic Gradient Descent and Support Vector Regression models continually improve their performance while the random forest model stops improving much earlier.
When the number of selected features is 46, the Stochastic Gradient Descent regression model achieves the best performance.
Experiments result show that the proposed approach performs significantly better than context-unaware method and the challenge baseline for all metrics, and it has the ability to discover a variety of temporal features that have underlying relationship with depression and further to build model on them.

(4)  Summarizing the results in Table \ref{tab9}, using the same dataset and modalities, the deep learning approach outperforms the traditional machine learning approach in most cases.
%In early studies of multiple audiovisual and text depression recognition based on deep learning, the LSTM is commonly used by different works to learn the discriminative patterns from both images and hand-crafted features.






\subsection{Others}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Most of the data of other modes in the auxiliary diagnosis of depression use single mode data, but a few studies combine data of multiple modes for prediction.
Each type of brain imaging can reflect the brain of patients with depression. For example, sMRI can clearly display the anatomical structure of the brain with high spatial resolution and reflect the internal structure of the brain, while fMRI can detect the changes of cerebral blood oxygen dependent level signals, and can reflect the active state of brain neural regions.
Therefore, Mousavian et al. combined the similarity of spatial cubes of sMRI and fMRI data, extracted features from them and input them into a unified machine learning classifier framework to detect depression~\cite{mousavian2021depression}.






\ifx\allfiles\undefined
\input{tnnls_suffix}
\fi
